# Databricks notebook source
# ============================================================
# Synthetic Stratification Demo (Databricks / PySpark)
# - Generates synthetic data with high NULL rates (matching your data)
# - Shows intentional "tiny strata explosion" failure mode
# - Runs Approach A and Approach B from strat_sampling.py
# - Runs validation and plots bin distributions
#
# Prereqs: you created these two files in your workspace:
#   - strat_sampling.py
#   - make_synth_data.py
# ============================================================

# COMMAND ----------

# If these modules are in the same workspace folder, regular import works.
# Otherwise, use %run to pull them in, or add to sys.path appropriately.
import strat_sampling as ss
import make_synth_data as ms

from pyspark.sql import functions as F

import pandas as pd
import matplotlib.pyplot as plt

# COMMAND ----------

# ============================================================
# CONFIG: Synthetic data + Stratification config for small run
# ============================================================

SYNTH = ms.SynthConfig(
    n=100_000,
    seed=7,
    # NULL rates roughly matching your real dataset:
    p_visa_null=0.30,
    p_visa_1_given_nonnull=0.45,
    p_null_balance=0.07,
    p_null_web=0.40,
    p_null_mobile=0.70,
    p_null_offers=0.33,
)

# For synthetic demo, smaller minimum stratum size
CFG_SYNTH = ss.StratConfig(
    min_stratum_size=5_000,
    control_frac=0.10,

    # keep it easy/understandable
    split_order=("visa_ind", "balance", "n_offers_year"),
    balance_bins_try=(5, 4, 3, 2),
    count_bins_try=(4, 3, 2),

    # Approach B start bins (moderate) + coarsen order
    b_balance_start=5,
    b_web_start=4,
    b_mobile_start=4,
    b_offers_start=4,
    coarsen_priority=("n_offers_year", "n_web_logins", "n_mobile_logins", "balance"),

    smd_thresh=0.10,
    max_prop_diff_thresh=0.02,
)

print(SYNTH)
print(CFG_SYNTH)

# COMMAND ----------

# ============================================================
# Generate synthetic Spark DataFrame
# ============================================================

DF = ms.make_synth_spark(spark, SYNTH).cache()
print("Rows:", DF.count())
DF.show(5, truncate=False)

# COMMAND ----------

# ============================================================
# Quick sanity checks: NULL rates + balance tail
# ============================================================

def null_rate(df, col):
    return df.select(F.avg(F.col(col).isNull().cast("double")).alias("null_rate")).collect()[0]["null_rate"]

cols = ["visa_ind", "balance", "n_web_logins", "n_mobile_logins", "n_offers_year"]
rates = {c: null_rate(DF, c) for c in cols}
display(pd.DataFrame({"col": list(rates.keys()), "null_rate": list(rates.values())}))

display(
    DF.selectExpr(
        "min(balance) as min_balance",
        "percentile_approx(balance, 0.5) as p50_balance",
        "percentile_approx(balance, 0.9) as p90_balance",
        "percentile_approx(balance, 0.99) as p99_balance",
        "max(balance) as max_balance"
    )
)

# COMMAND ----------

# ============================================================
# Plot helper: bar plot for bin counts (Spark -> pandas)
# ============================================================

def plot_bin_counts(df, bin_col, top_n=30, title=None, rotate=45):
    pdf = (
        df.groupBy(bin_col)
          .count()
          .orderBy(F.col("count").desc())
          .limit(top_n)
          .toPandas()
    )
    if pdf.empty:
        print(f"No data to plot for {bin_col}")
        return

    plt.figure()
    plt.bar(range(len(pdf)), pdf["count"].values)
    plt.xticks(range(len(pdf)), pdf[bin_col].astype(str).values, rotation=rotate, ha="right")
    plt.ylabel("count")
    plt.title(title or f"Top {top_n} bins for {bin_col}")
    plt.tight_layout()
    plt.show()

def plot_stratum_size_hist(df, stratum_col, title=None, bins=30):
    pdf = (
        df.groupBy(stratum_col)
          .count()
          .select(F.col("count").cast("double").alias("n"))
          .toPandas()
    )
    if pdf.empty:
        print(f"No strata to plot for {stratum_col}")
        return

    plt.figure()
    plt.hist(pdf["n"].values, bins=bins)
    plt.xlabel("stratum size")
    plt.ylabel("number of strata")
    plt.title(title or f"Histogram of stratum sizes: {stratum_col}")
    plt.tight_layout()
    plt.show()

# COMMAND ----------

# ============================================================
# INTENTIONAL FAILURE MODE:
# Create overly granular bins -> full cross-product -> tiny strata explosion
# ============================================================

df_bad = DF
df_bad = ss.add_visa_bin(df_bad, out_col="__BAD_visa", mode="3way")
df_bad = ss.add_quantile_bin(df_bad, "balance", out_col="__BAD_balance", n_bins=10)
df_bad = ss.add_quantile_bin(df_bad, "n_web_logins", out_col="__BAD_web", n_bins=8)
df_bad = ss.add_quantile_bin(df_bad, "n_mobile_logins", out_col="__BAD_mobile", n_bins=8)
df_bad = ss.add_quantile_bin(df_bad, "n_offers_year", out_col="__BAD_offers", n_bins=8)

df_bad = ss.make_stratum_key(
    df_bad,
    ["__BAD_visa", "__BAD_balance", "__BAD_web", "__BAD_mobile", "__BAD_offers"],
    out_col="stratum_bad"
).cache()

sizes_bad = df_bad.groupBy("stratum_bad").count().withColumnRenamed("count", "n").orderBy("n")
display(sizes_bad)

min_size = CFG_SYNTH.min_stratum_size
viol = sizes_bad.where(F.col("n") < min_size).count()
total = sizes_bad.count()
print(f"Bad stratification: {viol}/{total} strata below min_size={min_size} ({viol/total:.1%})")

# Plot: stratum size distribution for bad stratification
plot_stratum_size_hist(df_bad, "stratum_bad", title="BAD stratification: stratum size distribution", bins=40)

# Plot: bin distributions (top categories)
plot_bin_counts(df_bad, "__BAD_visa", top_n=10, title="BAD: visa bins")
plot_bin_counts(df_bad, "__BAD_balance", top_n=15, title="BAD: balance bins (top 15)")
plot_bin_counts(df_bad, "__BAD_web", top_n=15, title="BAD: web login bins (top 15)")
plot_bin_counts(df_bad, "__BAD_mobile", top_n=15, title="BAD: mobile login bins (top 15)")
plot_bin_counts(df_bad, "__BAD_offers", top_n=15, title="BAD: offers bins (top 15)")

# COMMAND ----------

# ============================================================
# BAD stratification validation (shows: validation != feasibility)
# ============================================================

df_bad2 = ss.assign_control_test(
    df_bad, id_col="cost_ID", stratum_col="stratum_bad",
    control_frac=CFG_SYNTH.control_frac, out_col="is_control_bad"
)

rep_bad, sum_bad = ss.validate_strata(
    df_bad2,
    stratum_col="stratum_bad",
    treat_col="is_control_bad",
    smd_thresh=CFG_SYNTH.smd_thresh,
    max_prop_diff_thresh=CFG_SYNTH.max_prop_diff_thresh,
)

display(sum_bad)
display(rep_bad.limit(50))  # top failures / smallest strata tend to show up early

# COMMAND ----------

# ============================================================
# Approach A (Tree stratification) + validation
# ============================================================

dfA, repA, sumA = ss.run_approach_A_with_validation(DF, CFG_SYNTH)

display(sumA)
display(repA)

# Plot stratum size distribution for Approach A
plot_stratum_size_hist(dfA, "stratum_A", title="Approach A: stratum size distribution", bins=30)

# Show top strata definitions (readable)
# (If you didnâ€™t include describe helpers in your version, skip this cell.)
try:
    display(ss.describe_stratum_A(dfA))
except Exception as e:
    print("describe_stratum_A not available in your strat_sampling.py; skipping.")

# Plot distributions of created A bin columns
a_bin_cols = [c for c in dfA.columns if c.startswith("__A_") and c.endswith("_bin")]
print("Approach A bin columns:", a_bin_cols)

for c in a_bin_cols:
    plot_bin_counts(dfA, c, top_n=20, title=f"Approach A: {c} (top 20)")

# COMMAND ----------

# ============================================================
# Approach B (Full bins + global coarsening) + validation
# ============================================================

dfB, repB, sumB = ss.run_approach_B_with_validation(DF, CFG_SYNTH)

display(sumB)
display(repB)

# Plot stratum size distribution for Approach B
plot_stratum_size_hist(dfB, "stratum_B", title="Approach B: stratum size distribution", bins=30)

# If describe helper exists:
try:
    display(ss.describe_stratum_B(dfB))
except Exception as e:
    print("describe_stratum_B not available in your strat_sampling.py; skipping.")

# Plot distributions of B bin columns
b_bin_cols = [c for c in dfB.columns if c.startswith("__B_")]
print("Approach B bin columns:", b_bin_cols)

for c in ["__B_visa", "__B_balance", "__B_n_web_logins", "__B_n_mobile_logins", "__B_n_offers_year"]:
    if c in dfB.columns:
        plot_bin_counts(dfB, c, top_n=20, title=f"Approach B: {c} (top 20)")

# COMMAND ----------

# ============================================================
# Quick "side by side" sanity: min stratum size achieved?
# ============================================================

def min_stratum_size(df, stratum_col):
    return df.groupBy(stratum_col).count().agg(F.min("count").alias("min_n")).collect()[0]["min_n"]

print("Min stratum sizes:")
print("  BAD:", min_stratum_size(df_bad, "stratum_bad"))
print("  A  :", min_stratum_size(dfA, "stratum_A"))
print("  B  :", min_stratum_size(dfB, "stratum_B"))

# COMMAND ----------

# ============================================================
# Notes:
# - Now swap DF with your real dataset and raise min_stratum_size to 280_000
# - Start with CFG_SIMPLE style split_order for Approach A
# - If you see tiny strata, Approach B will coarsen bins globally
# ============================================================
